{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sc.addPyFile('/local/path/to/sb/soft-boiled.zip')\n",
    "from src.algorithms import slp, gmm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_path = 'hdfs:///post_etl_datasets/twitter/year=2015'\n",
    "all_tweets = sqlCtx.read.parquet(data_path)\n",
    "all_tweets.registerTempTable('all_tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Pre-trained GMM model, filter \"poor\" points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gmm_model = gmm.load_model('/local/path/to/gmm/model.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gmm_model_filtered = {}\n",
    "\n",
    "percentile = 25\n",
    "error_at_Nth_percentile = np.percentile([gmm_model[word][1] for word in gmm_model],percentile)\n",
    "print 'Error (km) at Nth percentile:', error_at_Nth_percentile\n",
    "\n",
    "for word in gmm_model:\n",
    "    if gmm_model[word][1]<=error_at_Nth_percentile:\n",
    "        gmm_model_filtered[word] = gmm_model[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Saved SLP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "holdout_10pct = lambda (src_id) : src_id[-1] != '9'\n",
    "# Previously computed known User Locations -- result of slp.get_known_locs\n",
    "# Note: Dispersion threshold should be set very low and filter later in hyper parameter search\n",
    "locs_known = sc.pickleFile('hdfs:///path/to/slp/get_known_locs').cache()\n",
    "\n",
    "# Previously computed edge list -- result of slp.get_edge_list\n",
    "edge_list = sc.pickleFile('hdfs:///path/to/slp/get_edge_list').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Map side join by hand -- avoids having to shuffle all the data ever... \n",
    "#     This is an annoying, probably necessary optimization\n",
    "# Locatable Ids are ids in the edge list. Since edges are bi-directional (so src-> dst and dst-> src are both in edge list) \n",
    "# we can just keep distinct(src)\n",
    "locatable_ids_local = edge_list.map(lambda (src, (dst,weight)): src).distinct().collect()\n",
    "broadcast_set = sc.broadcast(set(locatable_ids_local))\n",
    "june_tweets = all_tweets.rdd.filter(lambda row: row != None and row.user!=None and row.user.id_str in broadcast_set.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Use predict user function to estimate user locations of \"locatable_ids\"\n",
    "# Note: This is very time consuming so save results to allow us to do further work on \n",
    "import datetime\n",
    "import itertools\n",
    "start_time = datetime.datetime.now()\n",
    "# Note: Set predict_lower bound to be very low and then filter later in hyper parameter search\n",
    "gmm_locations_no_filter2 = gmm.predict_user_gmm(sc, june_tweets,['user.location', 'text'], gmm_model_filtered, \\\n",
    "                                                radius=100, predict_lower_bound=.0001)\n",
    "gmm_locations_no_filter2.saveAsPickleFile('hdfs:///path/to/save/gmm/predict_user')\n",
    "elapsed_time = datetime.datetime.now() - start_time\n",
    "print elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Get known locations and broadcast for filter below\n",
    "locs_known_set_bcast = sc.broadcast(set(locs_known.map(lambda (id_str, loc_est): id_str).collect()))\n",
    "\n",
    "# Load saved results from above (even if we just calculated it) to force repartition \n",
    "gmm_locations_no_filter = sc.pickleFile('hdfs:///path/to/save/gmm/predict_user').coalesce(400)\n",
    "\n",
    "# Filter out locations that are already in locs known\n",
    "gmm_locations_no_filter_loc_est = gmm_locations_no_filter\\\n",
    "    .filter(lambda (id_str, loc_est): id_str not in locs_known_set_bcast.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Single Test of Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_iters': 6, 'known_threshold': 150, 'median': 12.601169254648408, 'dispersion_threshold': 250, 'num_locs': 256260, 'coverage': 0.10734800593147585, 'mean': 844.29700308525787}\n{'num_iters': 6, 'known_threshold': 150, 'median': 10.233307582508733, 'dispersion_threshold': 250, 'num_locs': 81609, 'coverage': 0.10003798600644537, 'mean': 442.57120792538876}\n"
     ]
    }
   ],
   "source": [
    "known_threshold = 150\n",
    "dispersion_threshold = 250\n",
    "gmm_percent_threshold = 0\n",
    "num_iters = 6\n",
    "\n",
    "# Filter known locs to only keep known locs below some dispersion threshold\n",
    "dispersion_filtered_locs = locs_known\\\n",
    "                .filter(lambda (id_str, loc_estimate): loc_estimate.dispersion < known_threshold)\n",
    "\n",
    "# If you need to filter based on percent chance that position is within some radius\n",
    "gmm_locations_filtered_loc_est = gmm_locations_no_filter_loc_est\\\n",
    "                    .filter(lambda (id_str, loc_est): loc_est.dispersion >gmm_percent_threshold)\n",
    "\n",
    "# Combine locations from gmm.predict_user and slp.get_known_locs\n",
    "unioned_locs_known  = dispersion_filtered_locs.union(gmm_locations_filtered_loc_est)\n",
    "\n",
    "# Apply holdout function\n",
    "filtered_locs_known = unioned_locs_known.filter(lambda (id_str, loc_estimate): holdout_10pct(id_str))\n",
    "\n",
    "# Train SLP \n",
    "estimated_locs = slp.train_slp(filtered_locs_known, edge_list, num_iters, dispersion_threshold=dispersion_threshold)\n",
    "# Test using both gmm.predict_user locations and slp.get_known_locs\n",
    "errors_all_test_point = slp.run_slp_test(unioned_locs_known, estimated_locs, holdout_10pct)\n",
    "errors_all_test_point['known_threshold'] = known_threshold\n",
    "errors_all_test_point['dispersion_threshold'] = dispersion_threshold\n",
    "errors_all_test_point['num_iters'] = num_iters\n",
    "print errors_all_test_point\n",
    "\n",
    "# Test using only points from slp.get_known_locs\n",
    "errrors_slp_known_locs_only = slp.run_slp_test(dispersion_filtered_locs, estimated_locs, holdout_10pct)\n",
    "errrors_slp_known_locs_only['known_threshold'] = known_threshold\n",
    "errrors_slp_known_locs_only['dispersion_threshold'] = dispersion_threshold\n",
    "errrors_slp_known_locs_only['num_iters'] = num_iters\n",
    "print errrors_slp_known_locs_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "results_all = []\n",
    "results_geo_only = []\n",
    "# Iterate over desired threshold for slp.get_known_locs\n",
    "for known_threshold in [50]:\n",
    "    # Get dispersion filtered locations \n",
    "    dispersion_filtered_locs = locs_known\\\n",
    "                .filter(lambda (id_str, loc_estimate): loc_estimate.dispersion < known_threshold)\n",
    "    # Iterate over dispersion threshold in slp.train\n",
    "    for dispersion_threshold in [50, 150, 250]:\n",
    "        # Iterate over gmm confidence gmm (GMMLocEstimate.prob)\n",
    "        for gmm_percent in [.1,.3, .5, .7, .85, .9, .95]:\n",
    "            # Union with GMM locations when available\n",
    "            gmm_locations_filtered_loc_est = gmm_locations_no_filter_loc_est\\\n",
    "                    .filter(lambda (id_str, loc_est): loc_est.dispersion >gmm_percent)\n",
    "            unioned_locs_known  = dispersion_filtered_locs.union(gmm_locations_filtered_loc_est)         \n",
    "            filtered_locs_known = unioned_locs_known.filter(lambda (id_str, loc_estimate): holdout_10pct(id_str))\n",
    "            # Test accuracy over number of SLP iterations\n",
    "            for num_iters in [1, 3, 5, 7, 9]:\n",
    "                print datetime.datetime.now(), known_threshold, dispersion_threshold, gmm_percent, num_iters\n",
    "                estimated_locs = train(filtered_locs_known, edge_list, num_iters, dispersion_threshold=dispersion_threshold)\n",
    "                errors_local = run_test(unioned_locs_known, estimated_locs, holdout_10pct)\n",
    "                errors_local['known_threshold'] = known_threshold\n",
    "                errors_local['dispersion_threshold'] = dispersion_threshold\n",
    "                errors_local['gmm_percent'] = gmm_percent\n",
    "                errors_local['num_iters'] = num_iters\n",
    "                results_all.append(errors_local)\n",
    "                \n",
    "                errors_local = run_test(dispersion_filtered_locs, estimated_locs, holdout_10pct)\n",
    "                errors_local['known_threshold'] = known_threshold\n",
    "                errors_local['dispersion_threshold'] = dispersion_threshold\n",
    "                errors_local['gmm_percent'] = gmm_percent\n",
    "                errors_local['num_iters'] = num_iters\n",
    "                results_geo_only.append(errors_local)\n",
    "\n",
    "print 'All Results: ', results_all\n",
    "print 'Results Geo Only: ', results_geo_only\n",
    "# Might want to save results since they take a while to compute"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "name": "python2",
   "language": "python"
  },
  "language_info": {
   "mimetype": "text/x-python",
   "nbconvert_exporter": "python",
   "version": "2.7.10",
   "name": "python",
   "file_extension": ".py",
   "pygments_lexer": "ipython2",
   "codemirror_mode": {
    "version": 2,
    "name": "ipython"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
